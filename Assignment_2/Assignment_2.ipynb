{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9d992ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32fa7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc835cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Natural language processing is a part of artificial intelligence\",\n",
    "    \"Machine learning and deep learning are subsets of AI\",\n",
    "    \"NLP uses machine learning techniques\",\n",
    "    \"Deep learning is powerful for NLP tasks\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "956877b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing part artificial intelligence',\n",
       " 'machine learning deep learning subsets ai',\n",
       " 'nlp uses machine learning techniques',\n",
       " 'deep learning powerful nlp tasks']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [w for w in tokens if w not in stop_words and w not in string.punctuation]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "processed_corpus = [preprocess(text) for text in corpus]\n",
    "\n",
    "processed_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac0ea39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>artificial</th>\n",
       "      <th>deep</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>part</th>\n",
       "      <th>powerful</th>\n",
       "      <th>processing</th>\n",
       "      <th>subsets</th>\n",
       "      <th>tasks</th>\n",
       "      <th>techniques</th>\n",
       "      <th>uses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ai  artificial  deep  intelligence  language  learning  machine  natural  \\\n",
       "0   0           1     0             1         1         0        0        1   \n",
       "1   1           0     1             0         0         2        1        0   \n",
       "2   0           0     0             0         0         1        1        0   \n",
       "3   0           0     1             0         0         1        0        0   \n",
       "\n",
       "   nlp  part  powerful  processing  subsets  tasks  techniques  uses  \n",
       "0    0     1         0           1        0      0           0     0  \n",
       "1    0     0         0           0        1      0           0     0  \n",
       "2    1     0         0           0        0      0           1     1  \n",
       "3    1     0         1           0        0      1           0     0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_matrix = count_vectorizer.fit_transform(processed_corpus)\n",
    "\n",
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    columns=count_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "bow_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae488e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>artificial</th>\n",
       "      <th>deep</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>part</th>\n",
       "      <th>powerful</th>\n",
       "      <th>processing</th>\n",
       "      <th>subsets</th>\n",
       "      <th>tasks</th>\n",
       "      <th>techniques</th>\n",
       "      <th>uses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ai  artificial      deep  intelligence  language  learning   machine  \\\n",
       "0  0.000000    0.166667  0.000000      0.166667  0.166667  0.000000  0.000000   \n",
       "1  0.166667    0.000000  0.166667      0.000000  0.000000  0.333333  0.166667   \n",
       "2  0.000000    0.000000  0.000000      0.000000  0.000000  0.200000  0.200000   \n",
       "3  0.000000    0.000000  0.200000      0.000000  0.000000  0.200000  0.000000   \n",
       "\n",
       "    natural  nlp      part  powerful  processing   subsets  tasks  techniques  \\\n",
       "0  0.166667  0.0  0.166667       0.0    0.166667  0.000000    0.0         0.0   \n",
       "1  0.000000  0.0  0.000000       0.0    0.000000  0.166667    0.0         0.0   \n",
       "2  0.000000  0.2  0.000000       0.0    0.000000  0.000000    0.0         0.2   \n",
       "3  0.000000  0.2  0.000000       0.2    0.000000  0.000000    0.2         0.0   \n",
       "\n",
       "   uses  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.2  \n",
       "3   0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_bow = bow_df.div(bow_df.sum(axis=1), axis=0)\n",
    "normalized_bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a522ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>artificial</th>\n",
       "      <th>deep</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>part</th>\n",
       "      <th>powerful</th>\n",
       "      <th>processing</th>\n",
       "      <th>subsets</th>\n",
       "      <th>tasks</th>\n",
       "      <th>techniques</th>\n",
       "      <th>uses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.453012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.35716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578303</td>\n",
       "      <td>0.35716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334067</td>\n",
       "      <td>0.41264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.41264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523381</td>\n",
       "      <td>0.523381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.41264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334067</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.41264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ai  artificial     deep  intelligence  language  learning  machine  \\\n",
       "0  0.000000    0.408248  0.00000      0.408248  0.408248  0.000000  0.00000   \n",
       "1  0.453012    0.000000  0.35716      0.000000  0.000000  0.578303  0.35716   \n",
       "2  0.000000    0.000000  0.00000      0.000000  0.000000  0.334067  0.41264   \n",
       "3  0.000000    0.000000  0.41264      0.000000  0.000000  0.334067  0.00000   \n",
       "\n",
       "    natural      nlp      part  powerful  processing   subsets     tasks  \\\n",
       "0  0.408248  0.00000  0.408248  0.000000    0.408248  0.000000  0.000000   \n",
       "1  0.000000  0.00000  0.000000  0.000000    0.000000  0.453012  0.000000   \n",
       "2  0.000000  0.41264  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.41264  0.000000  0.523381    0.000000  0.000000  0.523381   \n",
       "\n",
       "   techniques      uses  \n",
       "0    0.000000  0.000000  \n",
       "1    0.000000  0.000000  \n",
       "2    0.523381  0.523381  \n",
       "3    0.000000  0.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_corpus)\n",
    "\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "tfidf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "010407b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['natural', 'language', 'processing', 'part', 'artificial', 'intelligence'],\n",
       " ['machine', 'learning', 'deep', 'learning', 'subsets', 'ai'],\n",
       " ['nlp', 'uses', 'machine', 'learning', 'techniques'],\n",
       " ['deep', 'learning', 'powerful', 'nlp', 'tasks']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus = [word_tokenize(text) for text in processed_corpus]\n",
    "tokenized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f15f746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=50,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b931682c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0729393e-03,  4.7271003e-04,  1.0208104e-02,  1.8019602e-02,\n",
       "       -1.8605592e-02, -1.4234448e-02,  1.2918263e-02,  1.7946383e-02,\n",
       "       -1.0030984e-02, -7.5273751e-03,  1.4760993e-02, -3.0677442e-03,\n",
       "       -9.0746665e-03,  1.3108633e-02, -9.7204596e-03, -3.6306570e-03,\n",
       "        5.7542399e-03,  1.9837192e-03, -1.6570160e-02, -1.8896744e-02,\n",
       "        1.4623734e-02,  1.0140221e-02,  1.3516697e-02,  1.5253476e-03,\n",
       "        1.2702212e-02, -6.8102744e-03, -1.8939794e-03,  1.1537288e-02,\n",
       "       -1.5042618e-02, -7.8722741e-03, -1.5023078e-02, -1.8603110e-03,\n",
       "        1.9076865e-02, -1.4639303e-02, -4.6668286e-03, -3.8753832e-03,\n",
       "        1.6155630e-02, -1.1862010e-02,  9.1137852e-05, -9.5066773e-03,\n",
       "       -1.9205842e-02,  1.0013717e-02, -1.7519865e-02, -8.7845018e-03,\n",
       "       -7.1028109e-05, -5.9188530e-04, -1.5321815e-02,  1.9229794e-02,\n",
       "        9.9641699e-03,  1.8467069e-02], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv[\"learning\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf7b9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', 0.2105702906847),\n",
       " ('artificial', 0.16704507172107697),\n",
       " ('processing', 0.1501905620098114),\n",
       " ('uses', 0.1320112943649292),\n",
       " ('deep', 0.1267675906419754),\n",
       " ('subsets', 0.09985639154911041),\n",
       " ('nlp', 0.04236132651567459),\n",
       " ('part', 0.04067830368876457),\n",
       " ('powerful', 0.01243599783629179),\n",
       " ('intelligence', -0.012584488838911057)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar(\"learning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec739b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00045587, -0.00846928,  0.00525375,  0.00124259, -0.01240102,\n",
       "        0.00091135,  0.00447051, -0.00423807, -0.00955425, -0.00812436,\n",
       "        0.00554483, -0.00305503,  0.00275594, -0.00015909, -0.00178209,\n",
       "        0.00275119, -0.00223738,  0.00276977, -0.0077352 , -0.01319914,\n",
       "       -0.0014146 ,  0.00527331,  0.00963619,  0.00276856, -0.00388846,\n",
       "        0.00280395,  0.00035993, -0.00125929, -0.00166374,  0.00516055,\n",
       "        0.00418978, -0.00339041, -0.00539889, -0.0053815 , -0.00339299,\n",
       "       -0.002424  ,  0.00099349,  0.00302327, -0.00278685, -0.00287947,\n",
       "        0.01144264, -0.00337562, -0.00665261, -0.00104969,  0.006179  ,\n",
       "        0.0019327 , -0.00347674, -0.00811787,  0.00316158, -0.00293876],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def document_embedding(doc):\n",
    "    vectors = [word2vec_model.wv[word] for word in doc if word in word2vec_model.wv]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "doc_embedding = document_embedding(tokenized_corpus[0])\n",
    "doc_embedding\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
