{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5015c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26873186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3c4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
    "It enables machines to understand, interpret, and generate human language.\n",
    "Applications of NLP include chatbots, sentiment analysis, machine translation,\n",
    "speech recognition, information retrieval, and question answering systems.\n",
    "Machine learning and deep learning play a major role in modern NLP systems.\n",
    "\"\"\"\n",
    "\n",
    "tweet_text = \"\"\"\n",
    "Loving NLP and AI!!!\n",
    "Machine learning, deep learning, and data science are amazing\n",
    "Follow @OpenAI for more updates! #AI #NLP #MachineLearning\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c4943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization Output:\n",
      "\n",
      "['Natural', 'Language', 'Processing', '(NLP)', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence.', 'It', 'enables', 'machines', 'to', 'understand,', 'interpret,', 'and', 'generate', 'human', 'language.', 'Applications', 'of', 'NLP', 'include', 'chatbots,', 'sentiment', 'analysis,', 'machine', 'translation,', 'speech', 'recognition,', 'information', 'retrieval,', 'and', 'question', 'answering', 'systems.', 'Machine', 'learning', 'and', 'deep', 'learning', 'play', 'a', 'major', 'role', 'in', 'modern', 'NLP', 'systems.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "wt = WhitespaceTokenizer()\n",
    "wt_tokens = wt.tokenize(text)\n",
    "\n",
    "print(\"Whitespace Tokenization Output:\\n\")\n",
    "print(wt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe5801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-based Tokenization Output:\n",
      "\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'speech', 'recognition', ',', 'information', 'retrieval', ',', 'and', 'question', 'answering', 'systems', '.', 'Machine', 'learning', 'and', 'deep', 'learning', 'play', 'a', 'major', 'role', 'in', 'modern', 'NLP', 'systems', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "wpt = WordPunctTokenizer()\n",
    "wpt_tokens = wpt.tokenize(text)\n",
    "\n",
    "print(\"Punctuation-based Tokenization Output:\\n\")\n",
    "print(wpt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee791d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Tokenization Output:\n",
      "\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'speech', 'recognition', ',', 'information', 'retrieval', ',', 'and', 'question', 'answering', 'systems.', 'Machine', 'learning', 'and', 'deep', 'learning', 'play', 'a', 'major', 'role', 'in', 'modern', 'NLP', 'systems', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tbt = TreebankWordTokenizer()\n",
    "tbt_tokens = tbt.tokenize(text)\n",
    "\n",
    "print(\"Treebank Tokenization Output:\\n\")\n",
    "print(tbt_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206bd394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokenization Output:\n",
      "\n",
      "['Loving', 'NLP', 'and', 'AI', '!', '!', '!', 'Machine', 'learning', ',', 'deep', 'learning', ',', 'and', 'data', 'science', 'are', 'amazing', 'Follow', '@OpenAI', 'for', 'more', 'updates', '!', '#AI', '#NLP', '#MachineLearning']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "tweet_tokens = tt.tokenize(tweet_text)\n",
    "\n",
    "print(\"Tweet Tokenization Output:\\n\")\n",
    "print(tweet_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec9da52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWE Tokenization Output:\n",
      "\n",
      "['natural', 'language', 'processing', '(nlp)', 'is', 'a', 'fascinating', 'field', 'of', 'artificial', 'intelligence.', 'it', 'enables', 'machines', 'to', 'understand,', 'interpret,', 'and', 'generate', 'human', 'language.', 'applications', 'of', 'nlp', 'include', 'chatbots,', 'sentiment', 'analysis,', 'machine', 'translation,', 'speech', 'recognition,', 'information', 'retrieval,', 'and', 'question', 'answering', 'systems.', 'machine_learning', 'and', 'deep_learning', 'play', 'a', 'major', 'role', 'in', 'modern', 'nlp', 'systems.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "mwe = MWETokenizer(\n",
    "    [('machine', 'learning'), ('deep', 'learning'), ('artificial', 'intelligence')],\n",
    "    separator='_'\n",
    ")\n",
    "\n",
    "mwe_tokens = mwe.tokenize(text.lower().split())\n",
    "\n",
    "print(\"MWE Tokenization Output:\\n\")\n",
    "print(mwe_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25403438",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"running\", \"runs\", \"runner\", \"studies\", \"studying\",\n",
    "    \"easily\", \"fairly\", \"better\", \"connected\", \"connection\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c90f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer Output:\n",
      "\n",
      "running → run\n",
      "runs → run\n",
      "runner → runner\n",
      "studies → studi\n",
      "studying → studi\n",
      "easily → easili\n",
      "fairly → fairli\n",
      "better → better\n",
      "connected → connect\n",
      "connection → connect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(\"Porter Stemmer Output:\\n\")\n",
    "for w in words:\n",
    "    print(f\"{w} → {ps.stem(w)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2dbb972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowball Stemmer Output:\n",
      "\n",
      "running → run\n",
      "runs → run\n",
      "runner → runner\n",
      "studies → studi\n",
      "studying → studi\n",
      "easily → easili\n",
      "fairly → fair\n",
      "better → better\n",
      "connected → connect\n",
      "connection → connect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "ss = SnowballStemmer(\"english\")\n",
    "\n",
    "print(\"Snowball Stemmer Output:\\n\")\n",
    "for w in words:\n",
    "    print(f\"{w} → {ss.stem(w)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b411752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Output (Without POS):\n",
      "\n",
      "running → running\n",
      "better → better\n",
      "cars → car\n",
      "wolves → wolf\n",
      "children → child\n",
      "studies → study\n",
      "mice → mouse\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_words = [\n",
    "    \"running\", \"better\", \"cars\", \"wolves\",\n",
    "    \"children\", \"studies\", \"mice\"\n",
    "]\n",
    "\n",
    "print(\"Lemmatization Output (Without POS):\\n\")\n",
    "for w in lemma_words:\n",
    "    print(f\"{w} → {lemmatizer.lemmatize(w)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39c902c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with POS Tags:\n",
      "\n",
      "running (verb) → run\n",
      "better (adjective) → good\n",
      "studies (noun) → study\n",
      "studying (verb) → study\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatization with POS Tags:\\n\")\n",
    "\n",
    "print(\"running (verb) →\", lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
    "print(\"better (adjective) →\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(\"studies (noun) →\", lemmatizer.lemmatize(\"studies\", pos=\"n\"))\n",
    "print(\"studying (verb) →\", lemmatizer.lemmatize(\"studying\", pos=\"v\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7271f358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: Stemming vs Lemmatization\n",
      "\n",
      "running → Porter: run, Snowball: run, Lemma: run\n",
      "better → Porter: better, Snowball: better, Lemma: better\n",
      "studies → Porter: studi, Snowball: studi, Lemma: study\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparison: Stemming vs Lemmatization\\n\")\n",
    "\n",
    "for w in [\"running\", \"better\", \"studies\"]:\n",
    "    print(\n",
    "        f\"{w} → Porter: {ps.stem(w)}, \"\n",
    "        f\"Snowball: {ss.stem(w)}, \"\n",
    "        f\"Lemma: {lemmatizer.lemmatize(w, pos='v')}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
